P8105 HW5
================
Lucia Wang (lw3061)

## Question 1: WP unsolved homicides data

``` r
homicide = 
  read_csv("homicide-data.csv") |> 
  janitor::clean_names() |>
  separate(reported_date, into=c("year","month","day"), sep=c(4,6)) |>
  mutate(city_state = str_c(city, ", ", state))
```

The raw data has 52179 homicides between 2010 and 2016. It includes
information about the `reported_date` of murder, which I separated into
`year`,`month`, and `day`. Information about the victim’s name, age,
race, sex, and location are included, as well as the `disposition` or
status of the homicide case.

``` r
homi_status = homicide |> 
  mutate(status = case_when(
    disposition %in% c("Closed without arrest","Open/No arrest") ~ "unsolved",
    disposition == "Closed by arrest" ~ "solved"
  )) |>
  group_by(city_state, status) |> 
  summarize(number =n()) |>
  pivot_wider(names_from=status, values_from=number, values_fill =0) |>
  mutate(total = sum(solved, unsolved))

homi_status  |>
  select(-solved) |>
  knitr::kable()
```

| city_state         | unsolved | total |
|:-------------------|---------:|------:|
| Albuquerque, NM    |      146 |   378 |
| Atlanta, GA        |      373 |   973 |
| Baltimore, MD      |     1825 |  2827 |
| Baton Rouge, LA    |      196 |   424 |
| Birmingham, AL     |      347 |   800 |
| Boston, MA         |      310 |   614 |
| Buffalo, NY        |      319 |   521 |
| Charlotte, NC      |      206 |   687 |
| Chicago, IL        |     4073 |  5535 |
| Cincinnati, OH     |      309 |   694 |
| Columbus, OH       |      575 |  1084 |
| Dallas, TX         |      754 |  1567 |
| Denver, CO         |      169 |   312 |
| Detroit, MI        |     1482 |  2519 |
| Durham, NC         |      101 |   276 |
| Fort Worth, TX     |      255 |   549 |
| Fresno, CA         |      169 |   487 |
| Houston, TX        |     1493 |  2942 |
| Indianapolis, IN   |      594 |  1322 |
| Jacksonville, FL   |      597 |  1168 |
| Kansas City, MO    |      486 |  1190 |
| Las Vegas, NV      |      572 |  1381 |
| Long Beach, CA     |      156 |   378 |
| Los Angeles, CA    |     1106 |  2257 |
| Louisville, KY     |      261 |   576 |
| Memphis, TN        |      483 |  1514 |
| Miami, FL          |      450 |   744 |
| Milwaukee, wI      |      403 |  1115 |
| Minneapolis, MN    |      187 |   366 |
| Nashville, TN      |      278 |   767 |
| New Orleans, LA    |      930 |  1434 |
| New York, NY       |      243 |   627 |
| Oakland, CA        |      508 |   947 |
| Oklahoma City, OK  |      326 |   672 |
| Omaha, NE          |      169 |   409 |
| Philadelphia, PA   |     1360 |  3037 |
| Phoenix, AZ        |      504 |   914 |
| Pittsburgh, PA     |      337 |   631 |
| Richmond, VA       |      113 |   429 |
| Sacramento, CA     |      139 |   376 |
| San Antonio, TX    |      357 |   833 |
| San Bernardino, CA |      170 |   275 |
| San Diego, CA      |      175 |   461 |
| San Francisco, CA  |      336 |   663 |
| Savannah, GA       |      115 |   246 |
| St. Louis, MO      |      905 |  1677 |
| Stockton, CA       |      266 |   444 |
| Tampa, FL          |       95 |   208 |
| Tulsa, AL          |        0 |     1 |
| Tulsa, OK          |      193 |   583 |
| Washington, DC     |      589 |  1345 |

The above table shows the number of unsolved cases and the total number
of homicides per city.

``` r
balt_md_prop = homicide |>
  filter(city_state == "Baltimore, MD") |>
  mutate(status = case_when(
    disposition %in% c("Closed without arrest","Open/No arrest") ~ "unsolved",
    disposition == "Closed by arrest" ~ "solved"
  ))  |>
  group_by(status) |> 
  summarize(number =n()) |>
  pivot_wider(names_from=status, values_from=number, values_fill =0) |>
  mutate(total = sum(solved, unsolved)) |> select(unsolved,total)

balt_md = prop.test(balt_md_prop$unsolved, balt_md_prop$total) |>
  broom::tidy() |>
  janitor::clean_names()

balt_md |> select(estimate,conf_low, conf_high) |> knitr::kable()
```

|  estimate |  conf_low | conf_high |
|----------:|----------:|----------:|
| 0.6455607 | 0.6275625 | 0.6631599 |

The estimated proportion and confidence interval for unsolved homicides
in Baltimore, MD are above.

## Question 2: iterate the importing and tidying of data

First I used `list.files` to make a list of the file names. Then I
created a function that would read in a file and tidy it into the proper
format. Then I used `map` with this new function and the list of names,
and `bind_rows` to create the final dataframe.

``` r
names = list.files(path="data", full.names=TRUE)

readin_csv = function(path) {
  
  df =
    read_csv(path) |>
    janitor::clean_names() |>
    mutate(
      ident = path
    ) |>
  separate(ident, into=c("e", "arm","e2", "id", "csv"), sep=c(5,8,9,11)) |>
  arrange(arm, id) |>
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "value") |>
  separate(week, into=c("w", "week_no"), sep="_") |> 
  select(-e, -e2, -csv, -w)
  
}

output = map(names, readin_csv) |> bind_rows()
```

The following plot shows the results of subjects over time, comparing
the `con`trol group with the `ex`perimental group.

``` r
output |>
  mutate(week_num = as.numeric(week_no)) |>
  ggplot(aes(x=week_num, y=value, color=id)) + geom_line() + facet_grid(~arm)
```

![](p8105_hw5_lw3061_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

Overall, the experimental group saw large increases in their measured
values while the control group did not change much. There was a big jump
around week 1-2 for the experimental group which continued to increase
at a less steep rate to week 8. The control group seemed to fluctuate
more around the value of 0, with more inconsistencies in increasing or
decreasing behaviors.

## Question 3: simulate power in one-sample t-test

First, a function to calculate the mu_hat and p-value using the t-test
was created. Then a new dataframe using `map` was used to simulate this
function 5000 times.

``` r
sim_mean_ttest = function(mu) {
  data = tibble(
    x = rnorm(n=30, mean=mu, sd=5)
  )
  
  output = data |> 
    t.test() |> 
    broom::tidy() |>
    select(estimate, p.value) |>
    rename(mu_hat=estimate, pval=p.value)
}

sim_results = expand_grid(
  mu_df = c(0,1,2,3,4,5,6),
  iter = 1:50 #changing for now to avoid long knit times
) |>
  mutate(
    estimate = map(mu_df, sim_mean_ttest)
  ) |>
  unnest(estimate)
```

Next are plots of the simulated data.

``` r
sim_results |>
  group_by(mu_df) |>
  summarize(
    rej = sum(pval < 0.05),
    prop = rej/50 # see above comment about long knit times
  ) |>
  ggplot(aes(x=mu_df, y=prop)) + geom_line()
```

![](p8105_hw5_lw3061_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

As effect size increases, power also increases (the proportion of
estimates where the null was rejected increases). This is because the
difference needed for statistical significance is bigger so it becomes
more likely that you will reject the null.

``` r
true_df = sim_results |>
  group_by(mu_df) |>
  summarize(
    mean_mu = mean(mu_hat)
  )

rej_df = sim_results |>
  filter(pval < 0.05) |>
  group_by(mu_df) |>
  summarize(
    mean_mu = mean(mu_hat)
  )

ggplot(true_df, aes(x=mu_df, y=mean_mu)) + 
  geom_line() +
  geom_line(data=rej_df, color="red")
```

![](p8105_hw5_lw3061_files/figure-gfm/unnamed-chunk-8-1.png)<!-- --> In
this plot, the red line shows the association between true mu and
average estimate of mu-hat for only those samples where the null was
rejected, while the black line contains all samples. The sample average
of mu-hat for only the null-rejected data does not approximate the true
value of mu when the effect size is smaller, around 0-3. Around 3-4, the
red line starts to approach the black line and does approximate the true
value of mu as effect size increases.

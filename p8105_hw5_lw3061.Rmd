---
title: "P8105 HW5"
author: "Lucia Wang (lw3061)"
due date: "11-15-2023"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(purrr)
library(broom)

knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Question 1: WP unsolved homicides data
```{r}
homicide = 
  read_csv("homicide-data.csv") |> 
  janitor::clean_names() |>
  separate(reported_date, into=c("year","month","day"), sep=c(4,6)) |>
  mutate(city_state = str_c(city, ", ", state))
```

The raw data has `r nrow(homicide)` homicides between 2010 and 2016. It includes information about the `reported_date` of murder, which I separated into `year`,`month`, and `day`. Information about the victim's name, age, race, sex, and location are included, as well as the `disposition` or status of the homicide case. 

```{r}
homi_status = homicide |> 
  mutate(status = case_when(
    disposition %in% c("Closed without arrest","Open/No arrest") ~ "unsolved",
    disposition == "Closed by arrest" ~ "solved"
  )) |>
  group_by(city_state, status) |> 
  summarize(number =n()) |>
  pivot_wider(names_from=status, values_from=number, values_fill =0) |>
  mutate(total = sum(solved, unsolved))

homi_status  |>
  select(-solved) |>
  knitr::kable()
```
The above table shows the number of unsolved cases and the total number of homicides per city. 

```{r}
balt_md_prop = homicide |>
  filter(city_state == "Baltimore, MD") |>
  mutate(status = case_when(
    disposition %in% c("Closed without arrest","Open/No arrest") ~ "unsolved",
    disposition == "Closed by arrest" ~ "solved"
  ))  |>
  group_by(status) |> 
  summarize(number =n()) |>
  pivot_wider(names_from=status, values_from=number, values_fill =0) |>
  mutate(total = sum(solved, unsolved)) |> select(unsolved,total)

balt_md = prop.test(balt_md_prop$unsolved, balt_md_prop$total) |>
  broom::tidy() |>
  janitor::clean_names()

balt_md_est = balt_md |> select(estimate,conf_low, conf_high) 

balt_md_est |> knitr::kable()
```
The estimated proportion and 95% confidence interval for unsolved homicides in Baltimore, MD are in the table above.

```{r}
city_list = homicide |> group_by(city_state) |> select(city_state) |> summarize()

homi_function = function(name) {
  
  cityprop = homicide |>
    filter(city_state == name) |>
    mutate(status = case_when(
    disposition %in% c("Closed without arrest","Open/No arrest") ~ "unsolved",
    disposition == "Closed by arrest" ~ "solved") ) |>
    group_by(status) |> 
    summarize(number =n()) |>
    pivot_wider(names_from=status, values_from=number, values_fill =0) |>
    mutate(total = sum(solved, unsolved)) |> select(unsolved,total)
  
  city = prop.test(cityprop$unsolved, cityprop$total) |>
  broom::tidy() |>
  janitor::clean_names()
  
  output = city |> select(estimate,conf_low, conf_high) 
  
  output
  
}

homi_function("Charlotte, NC")
homi_function("Baltimore, MD")
homi_function("Buffalo, NY")

```

## Question 2: iterate the importing and tidying of data
First I used `list.files` to make a list of the file names. Then I created a function that would read in a file and tidy it into the proper format. Then I used `map` with this new function and the list of names, and `bind_rows` to create the final dataframe.
```{r}
names = list.files(path="data", full.names=TRUE)

readin_csv = function(path) {
  
  df =
    read_csv(path) |>
    janitor::clean_names() |>
    mutate(
      ident = path
    ) |>
  separate(ident, into=c("e", "arm","e2", "id", "csv"), sep=c(5,8,9,11)) |>
  arrange(arm, id) |>
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "value") |>
  separate(week, into=c("w", "week_no"), sep="_") |> 
  select(-e, -e2, -csv, -w)
  
}

output = map(names, readin_csv) |> bind_rows()

```

The following plot shows the results of subjects over time, comparing the `con`trol group with the `ex`perimental group.
```{r}
output |>
  mutate(week_num = as.numeric(week_no)) |>
  ggplot(aes(x=week_num, y=value, color=id)) + geom_line() + facet_grid(~arm)
```

Overall, the experimental group saw large increases in their measured values while the control group did not change much. There was a big jump around week 1-2 for the experimental group which continued to increase at a less steep rate to week 8. The control group seemed to fluctuate more around the value of 0, with more inconsistencies in increasing or decreasing behaviors. 

## Question 3: simulate power in one-sample t-test
First, a function to calculate the mu_hat and p-value using the t-test was created. Then a new dataframe using `map` was used to simulate this function 5000 times.
```{r}
sim_mean_ttest = function(mu) {
  data = tibble(
    x = rnorm(n=30, mean=mu, sd=5)
  )
  
  output = data |> 
    t.test() |> 
    broom::tidy() |>
    select(estimate, p.value) |>
    rename(mu_hat=estimate, pval=p.value)
}

sim_results = expand_grid(
  mu_df = c(0,1,2,3,4,5,6),
  iter = 1:50 #changing for now to avoid long knit times
) |>
  mutate(
    estimate = map(mu_df, sim_mean_ttest)
  ) |>
  unnest(estimate)
```

Next are plots of the simulated data.
```{r}
sim_results |>
  group_by(mu_df) |>
  summarize(
    rej = sum(pval < 0.05),
    prop = rej/50 # see above comment about long knit times
  ) |>
  ggplot(aes(x=mu_df, y=prop)) + geom_line()
```

As effect size increases, power also increases (the proportion of estimates where the null was rejected increases). This is because the difference needed for statistical significance is bigger so it becomes more likely that you will reject the null. 

```{r}
true_df = sim_results |>
  group_by(mu_df) |>
  summarize(
    mean_mu = mean(mu_hat)
  )

rej_df = sim_results |>
  filter(pval < 0.05) |>
  group_by(mu_df) |>
  summarize(
    mean_mu = mean(mu_hat)
  )

ggplot(true_df, aes(x=mu_df, y=mean_mu)) + 
  geom_line() +
  geom_line(data=rej_df, color="red")
```
In this plot, the red line shows the association between true mu and average estimate of mu-hat for only those samples where the null was rejected, while the black line contains all samples. The sample average of mu-hat for only the null-rejected data does not approximate the true value of mu when the effect size is smaller, around 0-3. Around 3-4, the red line starts to approach the black line and does approximate the true value of mu as effect size increases.

